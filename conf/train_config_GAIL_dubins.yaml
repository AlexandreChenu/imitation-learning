defaults:
  - _self_
  - algorithm: GAIL
  - optional optimised_hyperparameters: null

# General options
seed: 0
steps: 200000
env: GMazeDubins-v0
demo_file: C:\\Users\\alexandre.chenu\\Documents\\sr_dcil\\RF_DCIL_XPAG\\demos\\dubins_convert\\1.demo 
# Training/evaluation/logging hyperparameters
bc_pretraining:
  iterations: 0
  learning_rate: 0.00025
  weight_decay: 0
training:
  start: 1000
  interval: 1
  batch_size: 256
  learning_rate: 0.0003
  weight_decay: 0
evaluation:
  interval: 1000
  episodes: 1
logging:
  interval: 1000  # 0 to disable; logging too frequently can crash plotting/result in a large metrics file
# Agent/reinforcement learning hyperparameters
reinforcement:
  actor:
    hidden_size: 256
    depth: 2
    activation: relu
  critic:
    hidden_size: 256
    depth: 2
    activation: relu
  discount: 0.99
  target_temperature: -1
  polyak_factor: 0.995
memory:
  size: 1000000
# Imitation learning hyperparameters
imitation:
  trajectories: 0  # 0 to keep all trajectories; otherwise select number of trajectories
  subsample: 1  # Note that 20 was default in original GAIL implementation
  state_only: false
  absorbing: false
  mix_expert_data: none
  bc_aux_loss: false
# Miscellaneous options
check_time_usage: false
save_trajectories: false
render: false

hydra:
  job:
    chdir: true  # Change working directory to output directory (default behaviour for Hydra < 1.2)
  run:
    dir: F:\\alexandre.chenu\results\\imitation_baselines\\${algorithm}_${env}_${now:%m-%d_%H-%M-%S}  # Timestamp experiments up to second precision
  sweep:
    dir: F:\\alexandre.chenu\\results\\imitation_baselines\\${algorithm}_${env}_sweeper_${now:%m-%d_%H-%M-%S}
