defaults:
  - _self_
  - algorithm: SAC/ant
  - hyperparam_opt: empty
  - override hydra/sweeper: basic

seed: 0
steps: 1000000  # TODO: Change per environment: 2M for Adroit
replay:
  size: 1000000
pretraining:
  iterations: 0
  batch_size: 256
  learning_rate: 0.0001
  weight_decay: 0
reinforcement:
  model:
    hidden_size: 256
    depth: 2
    activation: 'relu'
  discount: 0.99
  target_temperature: -1
  learning_rate: 0.0003
  weight_decay: 0
  polyak_factor: 0.995
  max_grad_norm: 0  # 0 to disable; only applies to RL/BC updates
imitation:
  model:
    hidden_size: 128
    depth: 1
    activation: 'relu'
    reward_shaping: false
    reward_function: 'AIRL'
  trajectories: -1  # -1 to disable; otherwise select number of trajectories
  subsample: 20
  state_only: false
  absorbing: false
  learning_rate: 0.0003
  weight_decay: 0
training:
  start: 1000
  interval: 1
  batch_size: 256
evaluation:
  interval: 10000
  episodes: 30
  average_window: 5
save_trajectories: false
render: false
check_time_usage: false

# Run/sweep directories
hydra:
  run:
    dir: ./outputs/${env_type}_${algorithm}/${now:%m-%d_%H-%M-%S}
  sweep:
    dir: ./outputs/sweeper_${env_type}_${algorithm}/${now:%m-%d_%H-%M-%S}
